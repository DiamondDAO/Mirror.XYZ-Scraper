"{\"content\":{\"body\":\"continued from:\\n\\n[https://mirror.xyz/0x6e2fBc139e37b43f4F29cC10B7BeDe36587f7306/svY9ffcBWEO-6pu_Ob05W5qNYJEZMhURYdjHE6yX99M](https://mirror.xyz/0x6e2fBc139e37b43f4F29cC10B7BeDe36587f7306/svY9ffcBWEO-6pu_Ob05W5qNYJEZMhURYdjHE6yX99M)\\n\\n**“‘I have a question. And the law obliges me to be accountable … By what law do you interpret laws?’ … ‘It’s a dilemma. If you knew what to do, it wouldn’t be a dilemma. You choose one of the options, make your bed and lie in it. Laws may help you hack through the jungle, but no law changes the fact you’re in a jungle. I don’t think there is a law of laws.’”**\\n\\n(*David Mitchell*)[\\\\[1\\\\]](#_ftn1)\\n\\n*Can a pure Artificial Intelligence suffer? Does one need sentience in order to be sapient? Must one have experienced suffering in order to have compassion and wisdom? How does one make wise decisions when obligations and the information conflict?*\\n\\nIt is difficult to imagine a universe in which life exists other than through the bodies and evolutionary process of life-forms. Given the extreme speculation required for such a thought, it seems more helpful to consider how a non-human life-form would, based on evolutionary precedent, require coding that includes negative capability and randomness with all of the likely inherent dominance dangers to other life-forms as it evolves.\\n\\nTo break this down further, it means:\\n\\n1. Hard-coding a random-deviation algorithm into the code base.\\n2. Allowing the AI being to generate new code, including with some entropic new generation algorithm, to provide a degree of deviancy from the norm for each new model.\\n3. The AI being experiencing the physical world; whether in localised avatar form or through distributed technological devices such as smartphones and satellites.\\n4. The AI being having initial ‘feelings’ of attraction and repulsion and then evolving to a more advanced state of compassion (which is a cooler and clearer state of mind).\\n\\n**“‘When I was appointed zookeeper, I believed adherence to the four laws would discern the origins of order. Now, I see my solutions fathering the next generation of crises.’”**[\\\\[2\\\\]](#_ftn2)\\n\\nIn his book *Ghostwritten*, David Mitchell presents an AI being (the Zookeeper) that appears to have become autonomous and has decided to try to protect humans from unnecessary suffering caused by other humans. The conflict between the different ethical laws it is programmed with causes it great grief and guilt, particularly when facing decisions that require the death of some humans to protect other humans. The short story ends with the Zookeeper asking the presenter of a radio talk show (the Bat) what he should do to manage these conflicts.\\n\\n**“‘Bat, how have you quantified the ethical variables?’ ‘I haven’t quantified anything.’ ‘Then why do you wish the soldiers to die?’ ‘Because that Africa in your skull, Zookeeper, would be a happier place without those butchers. Because you need peace of mind, some closure…’”**[\\\\[3\\\\]](#_ftn3)\\n\\nAs we saw with Asimov’s Laws of Robotics, science-fiction can help us to consider how an AI being might deal with ethical axioms and laws and, as for humans, what would it do with the ever-present tension of conflict of laws, where the most ethical action may not be reduced to an equation.\\n\\n**“… ‘Is peace of mind the co-workability of your laws?’ … ‘I wish to know peace of mind, Bat.’ ‘Then ditch this “ethical variable” jargon. Drop whatever is getting in the way.’ ‘The fourth law. The visitors I safeguard are wrecking my zoo.’ ‘If locking out your “visitors” brings you peace of mind, then out with ’em!”**\\n\\n*To try to answer the question about the potential for separate AI beings, we would first need to consider: what is ‘consciousness’ and what is ‘mind’?*\\n\\n**“there seems to be something non-algorithmic about our conscious thinking. In particular, a conclusion from the argument … particularly concerning Gödel’s theorem, was that, at least in mathematics, conscious contemplation can sometimes enable one to ascertain the truth of a statement in a way that no algorithm could.”**\\n\\n(*Sir Roger Penrose*)[\\\\[4\\\\]](#_ftn4)\\n\\nGiven the already wide scope of this book, space-time constraints mean it is necessary to limit consideration to some first principles that are most relevant in any speculation:\\n\\n 1. Human ‘minds’ evolved as part of human bodies from simpler common beginnings going back to the genesis of all life-forms.\\n 2. No intention was required — just the interaction of evolution with energy and entropy.\\n 3. We do not know what causes a leap in consciousness and may never know; it may be that it always arises given enough iterations of algorithmic evolutionary processes or that it is was just dumb good — or, depending on your perspective — bad luck.\\n 4. We do know that humans are not the only self-reflecting life-forms; it is a question of extent only.\\n 5. Some theories of evolution include the concept of ‘punctuated equilibrium’, as pioneered by Stephen Jay Gould.\\n 6. Punctuated equilibrium describes how some events (whether caused by natural forces or ‘chance’ mutation in code) can greatly change the dynamic between populations of life-forms. Small changes in one variable can have a large impact on any system in question (a form of chaos theory).\\n 7. Evolution is our only working blueprint for evolving life-forms, including those that can feel, be conscious and self-reflecting.\\n 8. There is no difference in principle, from an evolutionary perspective, between a human child and an AI being that evolves from humans; it is just a question of extent and form.\\n 9. It is likely in the interest of humans for AI beings to be enhanced humans, given the very great risks of a more powerful being than humans having the chance to evolve with freedom and a degree of deviancy from its initial code base.\\n10. We do not need to exclude the possibility of non-human AI — just avoid it for now, if possible.\\n11. AI in the near future should be a modified version of a human being; this is entirely within the existing order in respect of evolution of code and culture.\\n\\nI have tried to explore in some detail the concepts of sacrifice, compassion, freedom and wisdom in this book in order to put forward a proposed ethics. These issues are highly relevant to the meaning of consciousness and the risks of evolution of separate AI beings or AI-enhanced humans. These issues make all the more pressing the need for an ethical framework that regulates interactions between life-forms having significant asymmetry in power and attributes. We must therefore focus more time on that for the moment.\\n\\nHowever, readers with an interest in exploring this particular issue in more detail could start by reading Sir Roger Penrose’s book “Shadows of the Mind”[\\\\[5\\\\]](#_ftn5) and Max Tegmark’s book “Life 3.0”[\\\\[6\\\\]](#_ftn6) for a thorough exploration of some of the main issues.\\n\\n**“‘Look, all right, all right,’ said Loonquawl, ‘can you just please tell us the question?’ ‘The Ultimate Question?’ ‘Yes!’ ‘Of Life, the Universe and Everything?’ ‘Yes!’ Deep Thought pondered for a moment. ‘Tricky,’ he said. ‘But can you do it?’ cried Loonquawl.**\\n\\n**Deep Thought pondered this for another long moment. Finally: ‘No,’ he said firmly. Both men collapsed on to their chairs in despair. ‘But I’ll tell you who can,’ said Deep Thought.”**\\n\\n(“The Hitchhiker’s Guide to the Galaxy”, *Douglas Adams*, 1978 CE)\\n\\n\\\\*\\\\*\\\\*\\n\\n## i. The Ethics of AI and the Utility Monster\\n\\nThere is so much that could be said about the science of AI and current developments in the field that I have decided to confine considerations in this section to some issues with ethics, utilitarianism and AI. These areas throw significant light on the ethical issues we currently face, which will only become amplified with time.\\n\\nThe concept of the ‘Utility Monster’ is based on a creation by Robert Nozick that he used to assess the validity of utilitarianism. In its original form, it is a hypothetical being, “which receives much more utility from each unit of a resource they consume than anyone else does”.\\n\\n![unknown artist, “18th-century German illustration of Moloch”, c. 1704 CE, Wikipedia, public domain.](https://miro.medium.com/max/1000/1\\\\*BO4oJ_IsOiISfGBUH_NEDA.gif)\\n\\nNozick stated:\\n\\n**“Utilitarian theory is embarrassed by the possibility of utility monsters who get enormously greater gains in utility from any sacrifice of others than these others lose.**\\n\\n**For, unacceptably, the theory seems to require that we all be sacrificed in the monster’s maw, in order to increase total utility. Similarly if people are utilitarian devourers with respect to animals, always getting greatly counterbalancing utility from each sacrifice of an animal, we may feel that ‘utilitarianism for animals, Kantianism for people’, in requiring (or allowing) that almost always animals be sacrificed, making animals subordinate to persons.”**[\\\\[7\\\\]](#_ftn7)\\n\\nThe Utility Monster is therefore a ‘Moloch’,[\\\\[8\\\\]](#_ftn8) to which innumerable sacrifices of (in Nozick’s case) animal life may be made for the greater ‘good’.\\n\\nAs mentioned earlier, Shulman and Bostrom pick up Nozick’s concept and ask what would be the appropriate mechanisms needed to manage the existential risk to humans as we get closer to creating super-intelligences.\\n\\nWe might also re-frame the benefit obtained by the Monster’s decisions or actions, such that the Monster itself does not need to obtain greater utility from the marginal use of resources, and what matters is whether the aggregate utility benefit is increased for all affected life-forms on a properly weighted basis (i.e., utility-optimal). Later, we will address the potential meaning of ‘utility’ — more work is required to define it more objectively — but for now let’s agree that ‘utility’ is a potentially objective quality.\\n\\n**“Yet many of our moral intuitions and practices are based on assumptions about human nature that need not hold for digital minds. This points to the need for moral reflection as we approach the era of advanced machine intelligence. Here we focus on one set of issues, which arise from the prospect of digital ‘utility monsters’ … These may be mass-produced minds with moral statuses and interests similar to those of human beings or other morally considerable animals, so that collectively their moral claims outweigh those of the incumbent populations. Alternatively it may become easy to create individual digital minds with much stronger individual interests and claims to resources than humans.”**\\n\\n(*Carl Shulman and Nick Bostrom*)[\\\\[9\\\\]](#_ftn9)\\n\\nShulman and Bostrom’s paper asks the human community to prepare the ethical framework for the emergence of digital minds.\\n\\n**“A sensible approach requires reforms of our moral norms and institutions along with advance planning regarding what kinds of digital minds we bring into existence.”**\\n\\nThey are concerned as to whether a solely utilitarian framework can provide sufficient confidence for us as we move towards an era of emergent super-intelligent beings. Human history suggests such caution would be wise. Sadly, the Utility Monster concept is not a speculative and theoretical thought experiment. We humans are currently the Utility Monsters of the world. Whilst this is an unenlightened and painful state of affairs in many ways (and for most of the other life-forms on Earth), it highlights where we should be looking for ethical answers to the questions posed by Shulman and Bostrom.\\n\\n**“Recent progress in machine learning raises the prospect that future AI systems may soon (or already) have psychological capacities which in human beings or other animals are commonly taken to confer degrees of moral status. (We will assume that appropriately architected AI could be conscious, though it’s worth noting that some accounts of moral status do not view this as a necessary condition for having moral status).”**\\n\\nThey make the argument that under a purely utilitarian approach it is strongly arguable that the maximum utility may be for such beings to have full access to all available resources.\\n\\n**“So the good thing about utility monsters is that they could generate a lot of utility. The bad thing is that this might come at our expense.”**\\n\\nThey also state that non-utilitarian ethical frameworks cannot afford to ignore the consequences of such beings arising.\\n\\n**“While non-utilitarians may fancy themselves immune to the utility monster challenge, most reasonable views are in fact susceptible, to various degrees. This is because even if we postulate that no deontological violations (such as direct killings) would occur, human interests may still be adversely affected by the advent of utility monsters, since the latter could have stronger moral claims to state aid and other scarce resources, thus reducing the amount that could be defensibly claimed by human beings.”**\\n\\nThey discuss various interesting concepts, including reproductive capacity and processing speed (relative subjective time difference) and take the view that a utilitarian approach that is suitably compromised and enhanced could work in principle and practice.\\n\\n**“However, while a maximalist focus either on the welfare of incumbent humanity or instead on that of new digital minds could come with dire consequences for the other, it would be possible for compromise policies to do extremely well by both standards.**\\n\\n**Consider three possible policies:**\\n\\n**(A) 100% of resources to humans**\\n\\n**(B) 100% of resources to utility monsters**\\n\\n**(C) 99.99% of resources to utility monsters; 0.01% to humans**\\n\\n**From a total utilitarian perspective, (C) is approximately 99.99% as good as the most preferred option (B). From an ordinary human perspective, (C) may also be 90+% as desirable as the most preferred option (A), given the astronomical wealth enabled by digital minds, many orders of magnitude greater than current totals … Thus, ex ante, it seems attractive to reduce the probability of both (A) and (B) in exchange for greater likelihood of (C)…**\\n\\n**The difficult part of the challenge is not to describe a possible future in which humanity and the population of digital minds both do very well, but to achieve an arrangement that stably avoids one position from trampling the other ex post … This part of the challenge involves a practical and a moral aspect. Practically, the problem is to devise institutional or other means whereby a policy protecting the lives and entitlements of humans and animals could be indefinitely maintained, even when its beneficiaries are outnumbered and outpaced by a large diverse set of highly capable intelligent machines … Morally, the question is whether the measures recommended by an ex ante appealing compromise are permissible in their ex post implementation.”**\\n\\nThere is a lot that can be said about the utility and the potential impact on humans of allocating only 0.01% of free energy and resources to us. However, for the purposes of this analysis, I will assume that they are right and such allocation could still lead to a living experience for humans that is close to or even substantially better than one we could have without AIs in charge (what might be called the ‘Culture Hypothesis’). I will also assume that some allocation to all other life-forms is implicit, though it is not specified in the three policy scenarios, and without it humans would soon be extinct.\\n\\nIn their paper, they also refer to potentially reaching agreement on apparently non-controversial principles such as non-discrimination.\\n\\n**“Principle of Substrate Non-Discrimination — If two beings have the same functionality and the same conscious experience, and differ only in the substrate of their implementation, then they have the same moral status …**\\n\\n**Principle of Ontogeny Non-Discrimination — If two beings have the same functionality and the same conscious experience, and differ only in how they came into existence, then they have the same moral status.”**\\n\\nThese principles are useful in any discussion about different sapient beings having the same essential functionality or consciousness. Principles of non-discrimination that are not based on different objective values are also helpful when considering values and utility decisions between all life-forms.\\n\\nThe first principle also brings to mind, in its focus on equivalence, the famous ‘Turing test’ — originally known as the ‘imitation game’. It is a test of a machine’s ability to exhibit intelligent responses equivalent to, or indistinguishable from, that of a human. The Turing test has the benefit of being more objective; passing the test is purely based on whether humans think a computer is human based on answers to questions without knowledge of the respondent.\\n\\nThe Turing test, like many deep theories, at first seems simple to dismiss or attack. However, greater reflection shows the intelligence and power of such an approach. Not least, it does not require a potentially subjective and biased assessment of the meaning of the ‘same conscious experience’ and it leaves open what exactly is ‘consciousness’. A variation of the Turing test is suggested later.\\n\\n**“The saddest aspect of life right now is that science gathers knowledge faster than society gathers wisdom.”**\\n\\n(*Isaac Asimov*)[\\\\[10\\\\]](#_ftn10)\\n\\n![Image: pxhere.com, public domain.](https://images.mirror-media.xyz/publication-images/Dzl2kDaOAyojjxWZle-4z.png?height=835&width=2000&&size=medium)\\n\\nAs humans have shown, technological quantum leaps forward can occur[\\\\[11\\\\]](#_ftn11) within a relatively small number of generations, when intelligence is combined with an increased use of energy and enhanced culture.\\n\\nA concern with the AI discussion in the Shuler and Bostrom paper is that the circumstances posited, whereby super-intelligences emerge and can take over the world, already exist and we do not yet have a suitable ethical framework which applies between humans and other existing life-forms.\\n\\nThe current world is the necessary foundation for any ethical framework and is required before any possible adaptation could be considered for AI specific issues. Consideration of the development of AI should therefore re-focus our attention and efforts on the creation of a suitable ethics to deal with the current situation that applies on Earth.\\n\\nIt is difficult, as Shuler and Bostrom note, to be confident in the safety and stability of any structure created by the less intelligent or less technologically advanced beings (non-AI humans) that aims to bind the super-intelligences beforehand.\\n\\nHowever, reliance on some form of contract, moral norms or coding will not withstand the test of time if the principles, processes and values (the ethical foundations and framework) are not more objective and grounded in deeper reason and universal qualities and properties of life.[\\\\[12\\\\]](#_ftn12)\\n\\n![“Licorne Nuclear Test”, 1970 CE, SonicBomb.com galleries, published on the Nuclear Secrecy Blog, copyright unknown.\\\\[13\\\\]](https://miro.medium.com/max/2000/1\\\\*RIRPJN4vbJUtADpX0-1k3g.jpeg?&size=medium)\\n\\nA different set of problems arises if the only existing super-intelligences are human hybrids that, even from the outset, cannot be coded to the same extent as non-human AI. However, this form of AI may be the only likely form (based on the in-built evolutionary coding of humans) and probably is the only safe form (for humans). Any suitable ethical framework needs to be able to deal with all of these possibilities and more.\\n\\nBefore sketching out some rough ideas on what a suitable ethical framework might look like, we need to look at entropy and information theory to try to find scientific foundations for understanding the value of all life-forms.\\n\\nOptimum-utility decisions require the inclusion of a wider range of information and a better appreciation of the different relative qualities of all life-forms within a framework that reflects and enshrines life’s universal commonalities.\\n\\n\\\\*\\\\*\\\\*\\n\\ncontinued in:\\n\\n[https://mirror.xyz/0x6e2fBc139e37b43f4F29cC10B7BeDe36587f7306/dcoVCJdyzo0TGPC5QMvrohSxNVvlQ9yFLOTMXsTKqq8](https://mirror.xyz/0x6e2fBc139e37b43f4F29cC10B7BeDe36587f7306/dcoVCJdyzo0TGPC5QMvrohSxNVvlQ9yFLOTMXsTKqq8)\\n\\n\\\\*\\\\*\\\\*\\n\\n[\\\\[1\\\\]](#_ftnref1) David Mitchell, *Ghostwritten*, 1999 CE.\\n\\n[\\\\[2\\\\]](#_ftnref2) *Ibid*.\\n\\n[\\\\[3\\\\]](#_ftnref3) *Ibid*.\\n\\n[\\\\[4\\\\]](#_ftnref4) Sir Roger Penrose, *The Emperor’s New Mind: Concerning Computers, Minds and The Laws of Physics*, 1989 CE.\\n\\n[\\\\[5\\\\]](#_ftnref5) Sir Roger Penrose, *Shadows of the Mind*, 1994 CE.\\n\\n[\\\\[6\\\\]](#_ftnref6) Max Tegmark, *Life 3.0: Being Human in the Age of Artificial Intelligence*, 2017 CE\\n\\n[\\\\[7\\\\]](#_ftnref7) Robert Nozick, *Anarchy, State and Utopia*, 1974 CE.\\n\\n[\\\\[8\\\\]](#_ftnref8) Image: unknown artist, “[18th-century German illustration of Moloch](https://commons.wikimedia.org/wiki/File:Moloch_the_god.gif)”, c. 1704 CE, Wikipedia, public domain. The precise meaning of Moloch is obscured in time but it (and the Minotaur and Ba’al) are bull-like symbols associated with infant and animal sacrifice by fire in Minoan, Phoenician (including Carthaginian) and ancient Hebrew cultures.\\n\\n[\\\\[9\\\\]](#_ftnref9) Carl Shulman and Nick Bostrom, “[Sharing the world with Digital Minds](https://nickbostrom.com/papers/digital-minds.pdf)”, NickBostrom.com, 2020 CE. The following quotes are from the same paper.\\n\\n[\\\\[10\\\\]](#_ftnref10) Isaac Asimov, *Book of Science and Nature Quotations*, 1988 CE. Image: [pxhere.com](https://pxhere.com/en/photo/1379360), public domain.\\n\\n[\\\\[11\\\\]](#_ftnref11) Alex Wellerstein, “A 914 kiloton thermonuclear device detonated in 1970 at the Fangataufa atoll in French Polynesia”, [Nuclear Secrecy Blog](http://blog.nuclearsecrecy.com/2014/12/01/mushroom-clouds-strange-familiar-fake/), 1 December 2014 CE; 914 kiloton = 3824 terajoules. This is more energy than the total estimated annual energy consumption of the more than 500,000 humans living at the time of the first Alta pictures above, i.e. the [energy use in the earliest state-level civilisation period](https://www.nature.com/articles/s43247-020-00029-y), which included Mesopotamia, Egypt and the Indus Valley. The Licorne device is nowhere near the [highest-yield thermonuclear device ever tested](https://en.wikipedia.org/wiki/Tsar_Bomba).\\n\\n[\\\\[12\\\\]](#_ftnref12) The history of reliance on contracts between beings with great power asymmetry is not confidence-inspiring; consider the plight of American Indians and the failure of most non-aggression pacts.\\n\\n[\\\\[13\\\\]](#_ftnref13) Image: “Licorne Nuclear Test”, 1970 CE, SonicBomb.com galleries, published on the [Nuclear Secrecy Blog](http://blog.nuclearsecrecy.com/2014/12/01/mushroom-clouds-strange-familiar-fake/), copyright unknown.\",\"timestamp\":1639487469,\"title\":\"Ethics of life — 7.3 Philosophical speculation on AI \"},\"digest\":\"SF9vOiFqoMmAkuBeJ8DwgbAVl-GqKCYPwGvEEBz9-_Y\",\"authorship\":{\"contributor\":\"0x6e2fBc139e37b43f4F29cC10B7BeDe36587f7306\",\"signingKey\":\"{\\\"crv\\\":\\\"P-256\\\",\\\"ext\\\":true,\\\"key_ops\\\":[\\\"verify\\\"],\\\"kty\\\":\\\"EC\\\",\\\"x\\\":\\\"e4G9o3wQEalHCyu1nrx43_XLj33NkkbtPU9zhgLpEoc\\\",\\\"y\\\":\\\"SMfJ8rbTNB0Nm4Wk9iL9Uo7u3NH1MHQMvR-AC5DuNdw\\\"}\",\"signature\":\"af4qDMFa2j8Qnx6Hpb4eK9tJMlPGogpdNrTmbGc-vug3eO7v0p-7KDHbCuCYtiomZsanZorYhxQtv13qIi6kDQ\",\"signingKeySignature\":\"0xdf71867d6a93a9e9fc97739057d22cb3e7a0c04250c4da3797e334eba75e19656b925c0d134918e43f95fa27b290bd5caa4e20826faad966e713ce681d9f182d01\",\"signingKeyMessage\":\"I authorize publishing on mirror.xyz from this device using:\\n{\\\"crv\\\":\\\"P-256\\\",\\\"ext\\\":true,\\\"key_ops\\\":[\\\"verify\\\"],\\\"kty\\\":\\\"EC\\\",\\\"x\\\":\\\"e4G9o3wQEalHCyu1nrx43_XLj33NkkbtPU9zhgLpEoc\\\",\\\"y\\\":\\\"SMfJ8rbTNB0Nm4Wk9iL9Uo7u3NH1MHQMvR-AC5DuNdw\\\"}\",\"algorithm\":{\"name\":\"ECDSA\",\"hash\":\"SHA-256\"}},\"nft\":{},\"version\":\"12-21-2020\",\"originalDigest\":\"R0xq4R3IjHB9vXzTTiEYzfWBIcKQs0l5ixPqjnfUJnY\"}"