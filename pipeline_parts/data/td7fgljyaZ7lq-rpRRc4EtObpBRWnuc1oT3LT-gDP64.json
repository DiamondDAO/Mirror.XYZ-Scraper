"{\"content\":{\"body\":\"*Editors Note: this article was originally published on Medium*\\n\\n## Design principles for AI\\n\\n**Guidance Through Intentional Friction**\\n\\nAI is a wonderful tool that has had many positive benefits, but it can also have a negative effect if users are not aware of the impact of their decisions.Users should never feel like enabling an AI tool is a set it and forget it action. They need to realize that some oversight will be needed and collaboration (through feedback and maintenance) will be a requirement to a successful implementation.\\n\\nAs designers, we can help mitigate uninformed decisions by identifying the personas who is tasked with enabling and maintaining the product and ensuring that they are fully aware of large decision points. Identify parts of your user journey where impactful decisions may appear deceptively easy and then intentionally slow down the users flow. Use alerts and education to amplify the importance of the action so users ask themselves “am I ready to take this step?” before proceeding.\\n\\nWhile it’s not an AI product, Mailchimp did a great job in a similar area through the use of an animation to emphasize the magnitude of the send campaign decision. It added a weightiness to the decision. In the world of AI we are often doing similar things where we are telling an ML model to automatically do something (potentially to thousands of users) and the person making that decision needs to be fully aware of the scale of the decision to start that flow.\\n\\n![](https://miro.medium.com/max/1400/1\\\\*x_88O53Wsof-o0oVgw4DRw.png)\\n\\n**Build Trust through Transparency**\\n\\nOnce a model is deployed it users will start seeing information surfaced through alerts, predictions on outcomes, or recommended actions to take to achieve a goal. This can be highly valuable, but will be totally wasted if a user doesn’t trust the AI.\\n\\nThis distrust is warranted due to the following problems\\n\\n* Many models suffer from the “black box” problem where predictions or recommended actions are surfaced with little explanation about why the user should believe it\\n* Models are often make wrong predictions, we are working in a field of probability :), or the data could be driving a inaccurate outcome.\\n\\nAs designers we should identify the areas we are asking the user to trust an automated action and increase transparency through explanations on the AI decisions. While the model that is being used may prevent us from giving full explanations (the black box issue) we can surface non-model explanations to help users make better informed decisions on whether or not to trust the prediction.\\n\\nBased on the persona the explanations we surface can be minimal (age of the prediction, the top factors) to complex (performance analytics, the model metrics, and training data).The ultimate goal is to give users the needed clarity to act confidently (or identify faulty data and model behavior).\\n\\n**AI is Collaborative**\\n\\nA model is only as good as the data it was trained with and an effective model can only do so much if it is deployed on low quality data. If your users are involved in any step of a model building or implementation process, they should be fully aware that what the decisions they are making has an impact on the AI behavior. The last thing you want is a user to face negative consequences because they were unaware that their decisions lead to a poorly trained or implemented model.\\n\\nDesigners should seek to emphasize this collaborative process through repeatedly showing the connection a users action has to the final output. This can be tricky if there are long delays in the feedback loop, but we should constantly look for areas where users actions impact the output and highlight the users responsibility and ownership.\\n\\n**Ensure Users Feel in Control**\\n\\nOne of the most common bits of feedback is that users will not trust a product if doesn’t feel controllable. In the world of B2B products, the people purchasing and adopting these products realize their job may be on the line if something goes wrong. So they need the assurance that they will have final control.\\n\\nAs designers we should define in advance what management and tracking tools the users will have to reassure them that they will continue to have ultimate control once their AI is deployed. This can be as simple as performance tracking and alerts with an on/off switch or as complex as A/B testing, model feedback loops and controls that allow them to investigate why something happened.\\n\\nIn the next post I will cover how I have learned to collaborate with devs and data scientists while building AI tools.\",\"timestamp\":1642622673,\"title\":\"UX Design for Artificial Intelligence (Pt 1) \"},\"digest\":\"GzC17rf5AqHqCLsBvCgp9RAUW2bLuwZXTpTYNybb-pc\",\"authorship\":{\"contributor\":\"0xcF4c23adc27d85Ac24325c7842BefD0DC5CDD426\",\"signingKey\":\"{\\\"crv\\\":\\\"P-256\\\",\\\"ext\\\":true,\\\"key_ops\\\":[\\\"verify\\\"],\\\"kty\\\":\\\"EC\\\",\\\"x\\\":\\\"H07tLwyWBR7swfwOhI5B4jqAbkYtcHgIz7cEN3Ice6Q\\\",\\\"y\\\":\\\"mX6ZTgkqu-RY0_A0-3Rqij8V26YcTjU6b4dJINiBpWQ\\\"}\",\"signature\":\"gFmFqJSk7cf8qsCdvC6L2YGg3cWKU0qCvR3iDJkG7ZKthnP0k9FuU_7K2EFCp7RzbSCxZzxPgkurLkwNhVtzXw\",\"signingKeySignature\":\"0x4d7cd003b2399d21bdd70da8b6c2fd8bf3b24cd7a0578fd8e6e813d7a03c569c374f645071d6bf3e3c0f04571a0198223ab651a1a9b8e2ae37bf90fde9b2e6951b\",\"signingKeyMessage\":\"I authorize publishing on mirror.xyz from this device using:\\n{\\\"crv\\\":\\\"P-256\\\",\\\"ext\\\":true,\\\"key_ops\\\":[\\\"verify\\\"],\\\"kty\\\":\\\"EC\\\",\\\"x\\\":\\\"H07tLwyWBR7swfwOhI5B4jqAbkYtcHgIz7cEN3Ice6Q\\\",\\\"y\\\":\\\"mX6ZTgkqu-RY0_A0-3Rqij8V26YcTjU6b4dJINiBpWQ\\\"}\",\"algorithm\":{\"name\":\"ECDSA\",\"hash\":\"SHA-256\"}},\"nft\":{},\"version\":\"12-21-2020\",\"originalDigest\":\"cIQ5_EFCh346GxSDb4CUn_aJZ-oZCJufPF7n1rY4QQs\"}"