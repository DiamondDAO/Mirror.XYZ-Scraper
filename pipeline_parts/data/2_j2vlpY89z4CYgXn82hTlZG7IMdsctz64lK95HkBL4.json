"{\"content\":{\"body\":\"1\\\\. Basic concept of consensus\\n\\nThe consensus algorithm can make the cluster work together, and can tolerate the failure of some member hosts. Usually, when we mention the failure of the host, we will distinguish between two situations: Byzantine failure and non Byzantine failure.\\n\\nBitcoin is the first decentralized system to solve Byzantine fault. Its method is to use workload proof consensus (POW). In a Byzantine system, not only the host crash will occur, but also some members may have malicious behavior to affect the decision-making process of the whole system.\\n\\nIf a distributed system can handle Byzantine faults, it can tolerate any type of errors. Common consensus algorithms supporting Byzantine faults include pow, POS, pbft and rbft.\\n\\nRaft can only handle non Byzantine faults, that is, raft consensus can tolerate faults such as system crash, network interruption / delay / packet loss. Common consensus algorithms or systems supporting non Byzantine faults include raft, Kafka, Paxos and zookeeper.\\n\\nSo why doesn't hyperledger fabric use a consensus mechanism that can tolerate Byzantine failures? Isn't that safer?\\n\\nOne reason is the design compromise between system complexity and security. Assuming that N nodes in a system may have Byzantine faults at the same time, Byzantine fault tolerance requires that the system has at least 3N + 1 nodes. For example, in order to deal with 100 potential malicious nodes, you need to deploy at least 301 nodes. This makes the system more complex. Raft only needs 2n + 1 nodes to deal with the potential non Byzantine failure of N nodes, which obviously has lower complexity and cost. Therefore, some distributed systems prefer raft, especially considering that in the licensed alliance chain environment such as hyperledger fabric, security mechanisms such as digital certificates are usually used to enhance security, so there is little possibility of malicious nodes.\\n\\n1. Fundamentals of raft consensus\\n\\nRaft is a distributed crash fault tolerant consensus algorithm, which can ensure that when some nodes in the system have non Byzantine faults, the system can still process the requests of the client. Technically speaking, raft is a consensus algorithm for managing the replicated log, which is an integral part of the replicated state machine (RSM).\\n\\nReplication state machine is a basic architecture for building distributed systems. Its main components include: node log containing command sequence, consensus module (such as raft) and state machine.\\n\\nThe working principle of replication state machine is as follows:\\n\\nThe client sends a request containing commands to the leader node. The leader node appends the received request to its log and sends the request to all follower nodes. The following node will also append the request to its own log and return a confirmation message. Once the leading node receives the confirmation messages from most of the following nodes, it will submit the command log to its managed state machine. Once the leading node submits the log, the following node will also submit the log to the state machine managed by itself. The leading node returns the response result to the client\\n\\nSo, what role does raft play in the replication state machine architecture?\\n\\nThe function of raft is to ensure that the logs of the following nodes are consistent with the logs of the leading nodes (i.e. log replication), so that the behavior of the whole distributed system looks consistent, and even if some nodes fail, it will not be affected.\\n\\nAnother question is, does the client need to know which is the dominant node?\\n\\nThe answer is No. the client can send a request to any node. If the node is the dominant node, it will directly process the request. Otherwise, the node will forward the request to the dominant node.\\n\\n1. Basic characteristics of raft consensus\\n\\n3\\\\.1 raft node status\\n\\nFor the raft algorithm, each node can only be in one of three states:\\n\\nFollow state: initially, all nodes are in follow state, that is, they are all follow nodes. Once a follower node does not communicate normally, it will change to candidate state, that is, it will become a candidate node. The log of the following node can be rewritten by the leading node.\\n\\nCandidate status: the node in the candidate status will initiate an election. If it receives the voting approval of most members of the cluster, it will change to the dominant state.\\n\\nDominant state: process client requests and ensure that all following nodes have the same log copy. The master node cannot overwrite its own log.\\n\\nIf the candidate node finds that the dominant node has been selected, it will return to the following state. Similarly, if the leading node finds that the term value of another leading node is higher, it will also return to the follow state.\\n\\nTerm is a monotonically increasing integer value used to identify the management cycle of the leading node. Each term begins at the election and ends before the next term.\\n\\n3\\\\.2 election of raft leading node\\n\\nRaft uses the heartbeat mechanism to start the election of dominant nodes. When the node starts, it enters the following state. As long as it can receive a valid RPC heartbeat message from the leading node or candidate node, it will remain in the following state. The dominant node will periodically send heartbeat messages (appendentries RPC messages without log entries) to all following nodes to maintain its dominance. If a following node does not receive a heartbeat message within a period of time, an election timeout event occurs. The node considers that there is no leading node and initiates an election to select a new leading node.\\n\\nTo start an election, the following node increments its current tenure value and transitions to the candidate state. The node first votes for itself, and then sends a message requesting voting (requestvote RPC message) to other nodes at the same time.\\n\\nThe candidate node will remain in the candidate state until the following events occur:\\n\\nThis node wins the election. Other nodes win the election. No node wins the election\\n\\nIf the node receives the voting approval of most nodes, it can win the election, and the node will transition to the dominant state and become a new dominant node. Note: each node can only cast one vote.\\n\\nIf other nodes announce that they are the leading node and have a higher tenure value, the node with a higher tenure value will become the new leading node:\\n\\nIf the votes of multiple candidate nodes are the same, there is no winning node.\\n\\nTo avoid this situation, you can reinitialize the election and ensure that the election timeout of each node is random, so as to avoid following the node into the candidate state at the same time.\\n\\n3\\\\.3 log replication\\n\\nOnce the dominant node is selected, it starts processing the client's request. The request contains commands that need to be executed by the replication state machine. The leading node appends the command to its own log, and then sends the appendentries RPC message to all following nodes in parallel to copy the new log entry. After the new log entry is safely copied, the leading node will execute the commands in the log entry on its state machine and return the results to the client.\\n\\nIf the following node crashes, runs slowly or loses packets in the network, the leading node will retry sending appendentries RPC messages indefinitely (even if it has returned response results to the client) until all following nodes finally get consistent log copies.\\n\\nWhen the appendentries RPC message is sent, the leading node will send the sequence number and tenure value of the preamble log entry of the new log entry at the same time. If the following node does not find the same sequence number and tenure value in its own log, it will reject the new log entry. Therefore, if pendentries are returned successfully, the leading node will know that the logs of the following nodes are completely consistent with itself.\\n\\nIn case of inconsistency, the leading node forces the following node to copy its own log.\\n\\n1. Implementation of raft sorting service for hyperledger fabric\\n\\nThe raft based sorting service replaces the previous Kafka sorting service. Each sorting node has its own raft replication state machine to submit logs. The client uses broadcast RPC to send transaction proposals. The raft sorting node generates new blocks based on consensus. When the peer node sends the deliver RPC, it sends the blocks to the peer node.\\n\\nThe workflow of the raft sorting node is as follows:\\n\\nThe transaction (such as proposal and configuration update) shall be automatically routed to the current dominant node of the channel. The dominant node shall check whether the configuration serial number verified by the transaction is consistent with the current configuration serial number. If not, the verification shall be performed, and the transaction shall be rejected after the verification fails. After verification, the leading node will transfer the received transaction to the ordered method of the block cutting module to create candidate blocks. If a new block is generated, the leading sorting node will apply it to the local raft finite state machine (FSM), which will try to copy it to a sufficient number of sorting nodes so that the submitted block can be written into the local ledger of the receiving node\\n\\nEach channel runs a separate instance of the raft protocol. In other words, a network with N channels has n raft clusters, and each raft cluster has its own dominant sorting node.\\n\\n1. Hyperledger fabric network practice based on raft consensus\\n\\nWe use the byfn component to show how to use the raft consensus module. Byfn contains 5 sorting nodes, 2 organizations, 4 peer nodes, and an optional CouchDB. The configuration of the raft sorting service is given in the configtx.yaml file.\\n\\nStart the default go chain code and raft consensus with the following script command, and the script will automatically generate necessary cryptographic data:\\n\\ncd fabric-samples/first-network\\n\\n./byfn.sh up -o etcdraft\\n\\nView sorting service:\\n\\ndocker logs -f [ordrer3.example.com](http://ordrer3.example.com)\\n\\nNow we verify the fault tolerance of raft.\\n\\nStop node3 first:\\n\\ndocker stop [orderer3.example.com](http://orderer3.example.com)\\n\\nThen stop node5:\\n\\ndocker stop [orderer5.example.com](http://orderer5.example.com)\\n\\nNow verify the effectiveness of the system, and you can see that the system can still respond to the client's request normally:\",\"timestamp\":1637322368,\"title\":\"In depth analysis of the latest raft sorting service of super ledger fabric\"},\"digest\":\"Ep-qcMMNkN8Saz3W-27KXqAsiEzc1rMLkS7yScuAzeQ\",\"authorship\":{\"contributor\":\"0x174459460A65Dac1242793534dAf9502B0168ab0\",\"signingKey\":\"{\\\"crv\\\":\\\"P-256\\\",\\\"ext\\\":true,\\\"key_ops\\\":[\\\"verify\\\"],\\\"kty\\\":\\\"EC\\\",\\\"x\\\":\\\"w79IF3E1_Xf0Nz9ofSVAouEuUaf6LWzvXiikPeQUAz4\\\",\\\"y\\\":\\\"elgVxAzSqJEGyCjbauAxTQTilCRbWyRHWVGvU1flLHo\\\"}\",\"signature\":\"JYeIYL094jk86QDuC0j0LTpodtvctLeyS6HKIdXo5_nDKNJkn-0kIvT5VwJFlESQD5I9w9BV2CHahE9R7DxLTg\",\"signingKeySignature\":\"0x542a4513bb555a7bb4d66250490c62622c199f92c66ebc491ac8247fb89ff6d167bde6639bc4be86f5c084d73aa216fb3739d80bae44da7803cfdb78e076548f1c\",\"signingKeyMessage\":\"I authorize publishing on mirror.xyz from this device using:\\n{\\\"crv\\\":\\\"P-256\\\",\\\"ext\\\":true,\\\"key_ops\\\":[\\\"verify\\\"],\\\"kty\\\":\\\"EC\\\",\\\"x\\\":\\\"w79IF3E1_Xf0Nz9ofSVAouEuUaf6LWzvXiikPeQUAz4\\\",\\\"y\\\":\\\"elgVxAzSqJEGyCjbauAxTQTilCRbWyRHWVGvU1flLHo\\\"}\",\"algorithm\":{\"name\":\"ECDSA\",\"hash\":\"SHA-256\"}},\"nft\":{},\"version\":\"12-21-2020\",\"originalDigest\":\"Ep-qcMMNkN8Saz3W-27KXqAsiEzc1rMLkS7yScuAzeQ\"}"