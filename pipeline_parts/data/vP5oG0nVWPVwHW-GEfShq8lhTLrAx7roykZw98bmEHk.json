"{\"content\":{\"body\":\"![Happy Trumpie](https://images.mirror-media.xyz/publication-images/G-EokRq1s-Ywj3nuTClHO.jpeg?height=1707&width=2560)\\n\\nLook, I’m not here to pick sides. Donald Trump is merely my scape-goat for my using of the term “fake news”, but whether or not you’re a Republican or Democrat, social-media junkie or forested hermit, you have to admit that our rapidly digitalizing, polarized world is running into a certain problem: **fake news**. Whether it’s Facebook and the whole spectrum of monolithic technology companies feeding us only information that we \\\\*want \\\\*to hear to increase advertising revenue, we are losing touch on diversified information, and as consumers, are facing the likelihood that whatever we are fed may not even be accurate. This is significant — given the average adult, according to *eMarketer*, spends 3 hours and 54 minutes per **day** on our phones. That’s a lot of time to be fed information we don’t even know is accurate. As a data scientist, my goal is to harness the good of the information age to catalyze change in society, and hopefully progress within this field and likeminded researchers over time will reduce the negatives.\\n\\nThis project, my self-dubbed “fake news classifier”, will aim to use natural language processing to classify whether a news is fake or not fake. First, let’s define fake news:\\n\\n![https://www.pnas.org/content/114/48/12631. Inspired by Dawn Graham’s own iteration of this project)](https://miro.medium.com/max/1400/0\\\\*bTM52c5nO6wGysWA)\\n\\nAs we see in the table above, fake news is prevalent over a wide array of mediums. For sake of use, we’ll be focusing on classifying satirical content (given that I, embarrassingly, have more than once, fallen to believing a The Onion news article).\\n\\nWe’ll be pulling from two Reddit subs, r/TheOnion and r/NotTheOnion to build our data. **The Onion** satisfies our requirement of satirical news, and **Not The Onion** is a subreddit which provides a diverse range of insane, but real news. Even Reddit frequenters often get these two mixed up.\\n\\n![](https://miro.medium.com/max/1600/1\\\\*6nxA2pd-5BYQj21aLy9GHA.png)\\n\\n![](https://miro.medium.com/max/1276/1\\\\*MyZ4S6ojPwb26yI60SxiNQ.png)\\n\\nThe Onion even has to issue a warning!\\n\\nWe can see from above that r/TheOnion has about 100,000 readers, while r/NotTheOnion has over 15 million (that’s roughly 150x more readers or a LOT more, for you non-math junkies out there). Given this fact, we can expect a ton more posts from Not The Onion, and likely a lot more diversity in terms of news sources — given that The Onion is literally its own publication. This may skew a little bit of the details, as we’ll see later on, as The Onion posts are likely written in a certain homogeneous way, and may affect our analysis, but for now we’ll forge on!\\n\\n## Data Collection and Exploratory Data Analysis\\n\\nFor our data collection, we used the Pushshift API (pushshift.io) to collect about 15,500 posts (the limit it hit) from r/TheOnion and 17,500 posts from r/NotTheOnion.\\n\\nYou can see how I used the PSAW wrapper (based off the Pushshift API) to extract the posts below:\\n\\n![](https://miro.medium.com/max/1400/1\\\\*x244PPoNoGqkaGUcKJfzcA.png)\\n\\nYou could definitely set time.sleep to be less than 1 second. I was playing it way too safe.\\n\\nTime spans for both Subreddits are as listed below:\\n\\n1. \\\\*\\\\*The Onion \\\\*\\\\*subreddit pulled posts all the way back from October 3, 2013 to July 5, 2019.\\n2. \\\\*\\\\*Not The Onion \\\\*\\\\*subreddit (17,500) posts pulled posts back only from March 29, 2019 to July 6, 2019.\\n\\nObjectively, the time span disparity between both Subreddits is not only due to the fact that Not The Onion has 150x more readers than The Onion, but also that The Onion is its own independent news site — while Not The Onion references a diverse range of “credible news sources”.\\n\\nAs we see above, The Onion mostly links The Onion related sites, while Not The Onion links relatively balanced news sources (surprisingly The Guardian is linked on both Subreddits). Looking at diversity of authors, we find something similar.\\n\\nThe author ‘dwaxe’ posts over 4,000 times — a pretty ridiculous number from the outset. Looking at the contrast between frequency of posts of The Onion and Not The Onion, the top poster in Not The Onion only has 350 posts. Considering the much higher number of readers, this may seem surprising, but makes sense when you consider the fact that the authors of The Onion subreddit are probably editors/posters from the publication itself.\\n\\nAll of this is to say: since the time disparity is large (a lot happens in a 6 year gap), and the domain linked of The Onion posts are mostly homogeneous, our model may not take into account an accurate representation of the current day. This is something we’ll definitely think about in future buildups of this project.\\n\\nIn addition, since these posts are all links to their own independent news articles, no post has a body — and thus what we will be focusing the entirety of our natural language analysis on are the titles of the articles. For the rest of the project, we’ll be imputing 1 to refer to **The Onion** and 0 to refer to **Not The Onion.**\\n\\n## Natural Language Processing\\n\\n## Sentiment Analysis\\n\\nTo add onto our list of features, I used the packages VADER and TextBlob (yes, yes, I know…) to conduct sentiment analysis on the titles. I’ll show you how I implemented these two packages — starting with VADER:\\n\\n```\\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzersia = SentimentIntensityAnalyzer()corpus = list(df['title'])\\nsentimentscores = []for i in corpus:\\n    score = sia.polarity_scores(i)\\n    score['title'] = i\\n    sentimentscores.append(score)\\n\\n    sentimentdf = pd.DataFrame(sentimentscores)\\nsentimentdf.drop(columns=['title'], inplace = True)\\n```\\n\\nVADER gives you four scores, as explained by its documentation:\\n\\n* The Positive, Negative and Neutral scores represent the proportion of text that falls in these categories. These scores add up to 1.\\n* The Compound score is a metric that calculates the sum of all the ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive).\\n\\nWe see our VADER scores below:\\n\\n![](https://miro.medium.com/max/778/1\\\\*XrOMJSiSDuCTNqTbfrE4ow.png)\\n\\nVADER Sentiment Scores\\n\\nAs we see, none of the scores are significantly different. But we can see that the compound score for **Not The Onion** posts are roughly 0.1 units less than that of the Onion — suggesting that the titles are a tad bit more negative.\\n\\nOur implementation of TextBlob sentiment analysis is about the same, so I’ll skip that (see <https://textblob.readthedocs.io/en/dev/> if you want to see more documentation), and go straight to business! TextBlob gives you two scores that range from 0 to 1:\\n\\n1. Polarity describes positive and negative emotions in the text, where 0 is most negative and 1 is most positive.\\n2. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. This range from 0 where it is most objective to 1 where it is most subjective.\\n\\nWe see our TextBlob sentiment scores below:\\n\\n![](https://miro.medium.com/max/760/1\\\\*975yoAX7VZ3mmYg2wICdOg.png)\\n\\nTextBlob Sentiment scores\\n\\nWe see some confirmations from the VADER analysis, with Not The Onion posts being slightly more negative, but also get some new information: subjectivity in The Onion posts is noticeably higher. We’ll see how sentiment analysis factors into our model later on.\\n\\n## Correlation\\n\\n![](https://miro.medium.com/max/794/1\\\\*8WQaTk4pAjLgQXGOiGDHlQ.png)\\n\\nWell. None of the scores are \\\\*that \\\\*correlated with each other — which makes sense considering this is a classification problem. But it’s helpful to know that sentiment analysis (especially that of Textblob) has some effect. Moving forward, we’ll use TextBlob’s sentiment analysis as our additional features.\\n\\n## Word Frequency\\n\\nBefore vectorizing, we use WordNetLemmatizer and RegexpTokenizer to process the titles. We can see this below:\\n\\n```\\nfor i in range(len(listoftitles)):\\n    tokenized = tokenizer.tokenize(listoftitles[i].lower())\\n    lemmatizelist = []\\n    for word in tokenizedi:\\n        lemmatizedword = lemmatizer.lemmatize(word)\\n        lemmatizelist.append(lemmatizedword)\\n    listoftitles[i] = \\\" \\\".join(lemmatizelist)soupedtitles = [BeautifulSoup(title).get_text() for title in listoftitles]cleanedsoup = [re.sub(\\\"[^a-zA-Z]\\\", \\\" \\\", i) for i in soupedtitles]arraysoup = np.ravel(cleanedsoup)df['title'] = arraysoup\\n```\\n\\nThe Tokenizer will separate our individual titles into a list of words, and the Lemmatizer will stem the word to its “root”. We then use regular expressions to remove any of the non-alphabetical characters, and replace the title array to our now processed title array!\\n\\nUsing CountVectorizer, we analyze the frequencies of words in both The Onion and Not The Onion subreddits:\\n\\n**Unigrams**\\n\\nThe top 5 unigrams that appear in The Onion are “life”, “new”, man”, “ha”, and “trump”.\\n\\nThe top 5 unigrams that appear in Not The Onion are “man”, “say”, “woman”, “trump”, and “police”.\\n\\n**Bigrams**\\n\\nThe top 5 bigrams that appear in The Onion are “year old”, “white house”, “donald trump”, “news source”, and “onion america”.\\n\\nThe top 5 bigrams that appear in Not The Onion are “year old”, “florida man”, “white house”, “molecule freedom”, and “police say”.\\n\\n**Stop Words**\\n\\nI take the intersection of the top unigrams and top bigrams that occur in both Subreddits, the word ‘onion’, and create a custom stop words list including the original English stop words list from Scikit-Learn. This will be useful not only in our modeling process but also in our coefficient analysis post-modeling.\\n\\n## Modeling\\n\\nI fit 7 models to the training set, with testing accuracy of the models being:\\n\\n1. Logistic Regression with Count Vectorizer (86.9%)\\n2. Logistic Regression with Count Vectorizer + Sentiment Analysis (80.6%)\\n3. Logistic Regression with TfidfVectorizer (88.6%)\\n4. SVC Model with TfidfVectorizer (88.4%)\\n5. Multinomial Naive Bayes with TfidfVectorizer (87.8%)\\n\\nThrough trial and test, I found that the best models involve TfidfVectorizer without sentiment analysis. The original models were quite overfit (with >98% accuracy on the training set). For my final 2 models, I wanted to remove the features that were zeroed-out by the Logistic Regression with the L1 penalty and fit them into more models to potentially fix overfitting.\\n\\n6\\\\. Gradient Boost Classifier (76%)\\n\\n7\\\\. Logistic Regression with TfidfVectorizer (85.8% on test set, and 91.9% on the training set — which was the **best model** and one I chose, taking into account the bias-variance trade-off, as well as it being the model with the best interpretability)\\n\\n## Model Selection\\n\\nWe chose our best model, taking into account the bias-variance tradeoff, to be our Logistic Regression with the TfidfVectorizer without sentiment analysis. With a train-test-split with a 10% test size, this achieved a 91.9% accuracy on the training set with a 85.8% accuracy on the testing data. Obviously this wasn’t the best testing accuracy, but it definitely reduced variance — which is also an aspect of good modeling.\\n\\nI plotted a Confusion Matrix of our model to our test scores:\\n\\n![](https://miro.medium.com/max/746/1\\\\*QVeEhXQFLJraC0wOdXuLQA.png)\\n\\nConfusion Matrix of Model\\n\\nWith the following metrics:\\n\\n```\\nSensitivity: 84.45%\\nSpecificity: 87.03%\\nPrecision: 85.16%\\nAccuracy: 85.82%\\nMisclassification Rate: 14.18%\\n```\\n\\nIt’s clear that there are a fair amount of false negatives in our test set prediction (which in this case, are posts that are predicted to be citing real news, but are actually fake news). This is definitely an issue we have to address going forward — even if it may sacrifice accuracy.\\n\\n## Coefficient Analysis\\n\\n![](https://miro.medium.com/max/856/1\\\\*8qJQFidp8udqAW8tqUGjtQ.png)\\n\\nOur most impactful features\\n\\nPlotting the top 10 features that affected our model in either direction, we see that:\\n\\nThe words ‘life’, ‘said’, ‘quiz’, ‘nation’, ‘news’, ‘blog’ and ‘incredible’ increased the odds of the post being in The Onion subreddit the most.\\n\\nThe words ‘arrested’, ‘police’, ‘florida’, ‘mueller report’, ‘cocaine’, and ‘jail’ increased the odds of the post being in Not The Onion subreddit the most.\\n\\nFeatures listed above affected the model’s classification ability a substantial amount. For instance, if the word ‘life’ was in the post’s title, relative to other words, the post had a 1531737x odds to be in The Onion. As we mentioned in our earlier discussion about the dataset, given that The Onion is drawing from homogeneous posters and domains (only The Onion news articles), it makes sense that certain words that are used often in either subreddit greatly affect the model’s classification capabilities.\\n\\n## Next Steps and Conclusion\\n\\nThis model was not perfect. There was still a fair bit of variance in the best model, with a high-enough bias, given the circumstances and the implications of not being able to detect fake news effectively. However, it was definitely better than the baseline, and considering that the model could detect Not The Onion posts with an accuracy of up to 88.5% accuracy makes it OK for my first voyage into Natural Language Processing and API web scraping.\\n\\nIf I were to do the project all over again, I would definitely gather more satirical news from a wider variety of sources than The Onion. Since the only text we could really analyze was the posts’ title (which is sadly fair, given that most of us in “The Internet Age” really only scan a news article’s title and make judgments from that), I would want to be able to create a database of news stories that extracted the actual news’ contents and made analyses based on that.\\n\\nAll things considered though, I definitely learned a ton through this project and can’t wait to continue building up my skills! For now, I’ll leave you all with these:\",\"timestamp\":1639473617,\"title\":\"Building a Fake News Classifier Using Natural Language Processing\"},\"digest\":\"DIZaNYhIw5tI5l0wHKDcdrnVpeXojrp-fzJyxoM3eTk\",\"authorship\":{\"contributor\":\"0xd88888f61A573eAF27B39e256FFaD017718a5434\",\"signingKey\":\"{\\\"crv\\\":\\\"P-256\\\",\\\"ext\\\":true,\\\"key_ops\\\":[\\\"verify\\\"],\\\"kty\\\":\\\"EC\\\",\\\"x\\\":\\\"S6ZzlYhLLB99s6cOeXiqXPA1kzBsY4jm3OJ72YoZLdA\\\",\\\"y\\\":\\\"Y0pe_4WxnMrJZ3Rb1HlDQscLVaeu6EJnsr5k6o6U-Yk\\\"}\",\"signature\":\"MI1W1oUQoygcAUDBaYxM-EUU3ki5rG0-P_0rpRx4TH6zKKFip0fVN9eSkW_WtWFvIJBebKI8wHo8dSlT6L-afg\",\"signingKeySignature\":\"0x1a573eb516d54c7f57e33ea645fc8f1e86be1bbfc968c608003b12962bcfc8f13628e8b6b0c21f1a8f890df6e0a77c634227039b84fc90b27c97a0999a6a180a1b\",\"signingKeyMessage\":\"I authorize publishing on mirror.xyz from this device using:\\n{\\\"crv\\\":\\\"P-256\\\",\\\"ext\\\":true,\\\"key_ops\\\":[\\\"verify\\\"],\\\"kty\\\":\\\"EC\\\",\\\"x\\\":\\\"S6ZzlYhLLB99s6cOeXiqXPA1kzBsY4jm3OJ72YoZLdA\\\",\\\"y\\\":\\\"Y0pe_4WxnMrJZ3Rb1HlDQscLVaeu6EJnsr5k6o6U-Yk\\\"}\",\"algorithm\":{\"name\":\"ECDSA\",\"hash\":\"SHA-256\"}},\"nft\":{},\"version\":\"12-21-2020\",\"originalDigest\":\"DIZaNYhIw5tI5l0wHKDcdrnVpeXojrp-fzJyxoM3eTk\"}"